%!TEX root = appunti.tex
%!TEX encoding = UTF-8 Unicode
\chapter{Distribuzioni discrete}
\begin{definition}[Distribuzione discreta]\label{def:distribuzione_discreta}
  Una distribuzione statistica le cui variabili possono avere solo valori discreti.

  Dunque, un numero aleatorio $X$ si dice con distribuzione discreta se la cardinalità di $I(X)$ è finita o numerabile.
\end{definition}
\begin{definition}[Distribuzione di probabilità]\label{def:distribuzione_di_probabilita}
  La funzione che descrive la probabilità che un certo valore si verifichi.
  \marginnote{La distribuzione di probabilità viene anche detta ``funzione di densità di probabilità''.}

  Se consideriamo $X$ una variabile aleatoria con distribuzione discreta, la sua distribuzione di probabilità sarà data da
  \[ \pr(X = x_i) = p(x_i) \text{ con } x_i \in I(X) \]
  dove
  \[ \sum_{x_i \in I(X)} \pr(X = x_i) = 1 \]
\end{definition}

\begin{definition}[Schema di Bernoulli]\label{def:schema_bernoulli}
  Una successione \( (E_i)_{i \in \mathbb{N}} \) di eventi stocasticamente indipendenti ed equiprobabili, ovvero tali per cui vale che
  \[ \pr(E_i) = p, ~ \forall i \in \mathbb{N} \]
\end{definition}

\section{Distribuzione binomiale} % (fold)
\begin{figure*}
  \includegraphics{distribuzione_binomiale}
  \caption{Distribuzione binomiale} 
\end{figure*}

\begin{definition}[Distribuzione binomiale]
  \label{def:distribuzione_binomiale}
  La distribuzione discreta della probabilità di ottenere $k$ successi su $n$ prove di Bernoulli.
  \footnote{ovvero dove il risultato è positivo con probabilità $p$ e negativo con probabilità \( (1-p) \), vedi definizione \ref{def:schema_bernoulli}}
\end{definition}

Dato uno schema di Bernoulli \( (E_i)_{i \in \mathbb{N}} \) con \( \pr(E_i) = p \),
\[ S_n = (E_1 + \ldots + E_n) \]
è il numero di successi su $n$ prove dove, ovviamente,
\[ I(S_n) = \{0,1, \ldots, n\} \]

La distribuzione di probabilità dunque è, tramite i costituenti:
\[ \pr(S_n = k) = \sum_{Q \subset (S_n = k)} \pr(Q) \]
ovvero dobbiamo sommare tutte le probabilità dei costituenti del primo tipo dell'evento \( (S_n = k) \), come, ad esempio
\[ Q = E_1 \cdots E_k \tilde{E}_{k+1} \cdots \tilde{E}_n \]
che rappresenta l'evento in cui i $k$ successi si sono ottenuti con le prime $k$ prove.
Analogamente ogni altro costituente di \( (S_n = k) \) conterrà $k$ eventi che si sono verificati e \( n-k \) che non si sono verificati.
Siccome tutti gli eventi sono indipendenti ed equamente distribuiti, ogni costituente $Q$ ha la stessa probabilità, pari a
\[ \pr(Q) = p^k (1-p)^{n-k} \]

Dunque per avere \( \pr(S_n = k) \), basta contare i costituenti.
Essi sono uguali al numero di modi di scegliere i $k$ eventi che si verificano sugli $n$ totali. Si ottiene quindi
\[
  \pr(S_n = k) = \binom{n}{k} p^k (1-p)^{n-k} 
\]

Si dice dunque che \( S_n \) ha distribuzione binomiale \( \mathcal{B}(n, p) \) dove $n$ è il numero degli eventi e $p$ la probabilità di ognuno.

\newthought{Calcoliamo infine la previsione di \( S_n \)}:
\begin{align*}
  \pr(S_n) &= \pr(E_1 + \ldots + E_n) \\
  &= \sum_{i=1}^n \pr(E_i) \\
  &= np
\end{align*}

\newthought{e la sua varianza}:
\begin{align*}
  \var(S_n) &= \var(E_1 + \ldots + E_n) \\
  &= \sum_{i = 1}^n \var(E_i) \\
  &= \sum_{i = 1}^n \pr(E_i^2) - \pr(E_i)^2 \\
  &= \sum_{i = 1}^n \pr(E_i) - \pr(E_i)^2 \\
  &= \sum_{i = 1}^n p - p^2 \\
  &= \sum_{i = 1}^n p (1 - p) \\
  &= n p (1 - p)
\end{align*}

% TODO 'moda' della distribuzione

% section distribuzione_binomiale (end)

\section{Distribuzione ipergeometrica} % (fold)
\begin{figure*}
  \includegraphics{distribuzione_ipergeometrica}
  \caption{Distribuzione ipergeometrica} 
\end{figure*}

\begin{definition}[Distribuzione ipergeometrica]
  \label{def:distribuzione_ipergeometrica}
  La distribuzione discreta della probabilità di ottenere \( i \) successi su \( n \) campioni, date \( H \) possibilità di successo su \( N \) possibilità totali.
\end{definition}
Consideriamo \( H \) modi per una ``buona'' selezione ed \( N - H \) per una ``cattiva'' selezione, su un totale di \( N \) possibilità.
Prendiamo \( n \) campioni e sia
\[ E_i = (\text{abbiamo una ``buona'' selezione all'}i\text{-esimo campione.}) \]
Sia \( X \) il numero aleatorio che conta il numero di buone selezioni:
\[
  X = \sum_{i = 1}^n E_i
\]

La probabilità di \( i \) selezioni di successo allora è
\begin{align*}
  \pr(X = i) &= \frac{[\text{\# modi per }i\text{ successi}][\text{\# modi per }n - i\text{ fallimenti}]}{[\text{\# casi possibili}]} \\
  &= \frac{\displaystyle \binom{H}{i} \binom{N-H}{n-i}}{\displaystyle \binom{N}{n}}
\end{align*}

Si dice dunque che \( X \) possiede distribuzione ipergeometrica.

\newthought{La probabilità dell'evento \( E_i \)} è data da:
\begin{align*}
  \pr(E_i) &= \frac{\text{\# casi favorevoli}}{\text{\# casi possibili}} \\
  &= \frac{H D_{n-1}^{N-1}}{D_n^N} \\
  &= \frac{H (N-1)(N-2) \cdots (N-n+1)}{N(N-1)(N-2) \cdots (N-n+1)} \\ % DOUBT qualcosa non torna...
  &= \frac{H}{N}
\end{align*}
dove \( D_k^n \) è una disposizione semplice\footnote{vedi definizione \ref{def:disposizione_semplice}} di \( k \) elementi su \( n \) possibili.

\newthought{Dunque la previsione di \( X \) è}:
\begin{align*}
  \pr(X) &= \sum_{i=1}^n \pr(E_i) \\
  &= n \frac{H}{N}
\end{align*}

E la sua varianza è:
\begin{align*}
  \var(S_n) &= \var \left(\sum_{i = 1}^n E_i\right) \\
  &= \sum_{i = 1}^n \var(E_i) + \sum_{i,j ~ i \neq j} \cov(E_i, E_j) \\ % DOUBT non ho la più pallida idea del perché
  &= n \frac{H}{N} \left( 1 - \frac{H}{N} \right)
\end{align*}
% section distribuzione_ipergeometrica (end)

\section{Distribuzione geometrica} % (fold)
\begin{figure*}
  \includegraphics{distribuzione_geometrica}
  \caption{Distribuzione geometrica} 
\end{figure*}

\begin{definition}[Distribuzione geometrica]\label{def:distribuzione_geometrica}
  La distribuzione discreta della probabilità di ottenere il primo successo all'istante $i$ in una serie di prove.
\end{definition}

Dato uno schema di Bernoulli \( E_i \) ed un numero aleatorio \( T \) che rappresenta l'istante del primo successo in una serie di prove, ovvero \( T = \inf\{ n | E_n = 1 \} \).
\[
  I(T) = \mathbb{N} \setminus \{ 0 \}
\]
\[
  (T = i) = \tilde{E}_1 \cdots \tilde{E}_{i-1} E_i
\]

La distribuzione discreta di probabilità è
\begin{align*}
  \pr(T = i) &= \pr(\tilde{E}_1 \cdots \tilde{E}_{i-1} E_i) \\
  &= \pr(\tilde{E}_1) \cdots \pr(\tilde{E}_{i-1}) \pr(E_i) \\
  &= (1-p)^{i-1} p  
\end{align*}

Si dice che \( T \) ha distribuzione geometrica di parametro \( p \).
Utilizzando la somma della serie geometrica, si verifica che
\[
  \sum_{i=1}^{+ \infty} \pr(T = i) = 1
\]

\newthought{Possiamo dunque calcolare la previsione di \( T \)}:
\[
  \pr(T) = \frac{1}{p}
\]

% section distribuzione_geometrica (end)
